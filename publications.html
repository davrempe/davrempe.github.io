<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Davis Rempe</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/1-col-portfolio.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">About</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="publications.html">Research</a>
                    </li>
                    <li>
                        <a href="cv.html">CV</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

        <!-- Page Heading -->
        <div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">Publications
                    <!--<small>Secondary Text</small> -->
                </h1>
            </div>
        </div>
        <!-- /.row -->

        <div class="row">
            <div class="col-md-8">
                <h3>NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</h3>
                <h4>N. Kulkarni, <b>D. Rempe</b>, K. Genova, A. Kundu, J. Johnson, D. Fouhey, L. Guibas. <b>arXiv</b>, 2023.</h4>
                <p>We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key 
                    idea is to create a neural interaction field attached to a specific object, which outputs the distance to the 
                    valid interaction manifold given a human pose as input. This interaction field guides the sampling of an 
                    object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To 
                    support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, 
                    we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific 
                    anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic
                    data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches 
                    in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for 
                    Trajectory sYnthesis.</p>
                <a class="btn btn-primary" href="https://nileshkulkarni.github.io/nifty/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://nileshkulkarni.github.io/nifty/assets/paper.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://nileshkulkarni.github.io/nifty/assets/paper.pdf">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/nifty_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Language-Guided Traffic Simulation via Scene-Level Diffusion</h3>
                <h4>Z. Zhong, <b>D. Rempe</b>, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone, and B. Ray. <b>arXiv</b>, 2023.</h4>
                <p>Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) 
                    development. However, current approaches for controlling learning-based traffic models require significant domain expertise
                     and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model
                      that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic 
                      and controllable traffic model backbone, and an effective method to interface with a traffic model using language. 
                      To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer
                       backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert 
                       a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through 
                       comprehensive evaluation, we demonstrate the effectiveness of our proposed method in generating realistic, 
                       query-compliant traffic simulations.</p>
                <!-- <a class="btn btn-primary" href="https://sites.google.com/stanford.edu/copilot">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a> -->
                <a class="btn btn-primary" href="https://arxiv.org/abs/2306.06344">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://arxiv.org/abs/2306.06344">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/ctgpp_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>COPILOT: Human-Environment Collision Prediction and Localization from Multi-view Egocentric Videos</h3>
                <h4>B. Pan, B. Shen, <b>D. Rempe</b>, D. Paschalidou, K. Mo, Y. Yang, and L. Guibas. International Conference on Computer Vision (<b>ICCV</b>), 2023.</h4>
                <p>The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance 
                    in applications such as VR, AR, and wearable assistive robotics. In this work, we introduce the challenging problem of 
                    predicting collisions in diverse environments from multi-view egocentric videos captured from body-mounted cameras. 
                    Solving this problem requires a generalizable perception system that can classify which human body joints will collide 
                    and estimate a collision region heatmap to localize collisions in the environment. To achieve this, we propose a 
                    transformer-based model called COPILOT to perform collision prediction and localization simultaneously, which accumulates 
                    information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and 
                    enable future research on this task, we develop a synthetic data generation framework that produces egocentric videos of 
                    virtual humans moving and colliding within diverse 3D environments. This framework is then used to establish a large-scale
                     dataset consisting of 8.6M egocentric RGBD frames. Extensive experiments show that COPILOT generalizes to unseen synthetic 
                     as well as real-world scenes. We further demonstrate COPILOT outputs are useful for downstream collision avoidance through 
                     simple closed-loop control.</p>
                <a class="btn btn-primary" href="https://sites.google.com/stanford.edu/copilot">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://arxiv.org/abs/2210.01781">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://arxiv.org/abs/2210.01781">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/copilot_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion</h3>
                <h4><b>D. Rempe</b>, Z. Luo, X.B. Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler, and O. Litany. Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</h4>
                <p>We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet 
                    user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories,
                     which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories 
                     through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This 
                     trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian 
                     animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the 
                     value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for 
                     particular scenarios such as collision avoidance and traversing uneven terrain.</p>
                <a class="btn btn-primary" href="https://nv-tlabs.github.io/trace-pace/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://nv-tlabs.github.io/trace-pace/docs/trace_and_pace.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://nv-tlabs.github.io/trace-pace/">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/trace_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>CurveCloudNet: Processing Point Clouds with 1D Structure</h3>
                <h4>C. Stearns, J. Liu, <b>D. Rempe</b>, D. Paschalidou, J. Park, S. Mascha, and L. Guibas. <b>arXiv</b>, 2023.</h4>
                <p>Modern depth sensors such as LiDAR operate by sweeping laser-beams across the scene, resulting in a point cloud with notable 
                    1D curve-like structures. In this work, we introduce a new point cloud processing scheme and backbone, called CurveCloudNet, 
                    which takes advantage of the curve-like structure inherent to these sensors. While existing backbones discard the rich 1D 
                    traversal patterns and rely on Euclidean operations, CurveCloudNet parameterizes the point cloud as a collection of polylines 
                    (dubbed a "curve cloud"), establishing a local surface-aware ordering on the points. Our method applies curve-specific operations
                     to process the curve cloud, including a symmetric 1D convolution, a ball grouping for merging points along curves, and an 
                     efficient 1D farthest point sampling algorithm on curves. By combining these curve operations with existing point-based operations, 
                     CurveCloudNet is an efficient, scalable, and accurate backbone with low GPU memory requirements. Evaluations on the ShapeNet, 
                     Kortx, Audi Driving, and nuScenes datasets demonstrate that CurveCloudNet outperforms both point-based and sparse-voxel backbones 
                     in various segmentation settings, notably scaling better to large scenes than point-based alternatives while exhibiting better 
                     single object performance than sparse-voxel alternatives.</p>
                <!-- <a class="btn btn-primary" href="https://sites.google.com/stanford.edu/copilot">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a> -->
                <a class="btn btn-primary" href="https://arxiv.org/abs/2303.12050">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://arxiv.org/abs/2303.12050">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/curve_cloud_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Guided Conditional Diffusion for Controllable Traffic Simulation</h3>
                <h4>Z. Zhong, <b>D. Rempe</b>, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone. International Conference on Robotics and Automation (<b>ICRA</b>), 2023.</h4>
                <p>Controllable and realistic traffic simulation is critical for developing and verifying autonomous vehicles. 
                    Typical heuristic-based traffic models offer flexible control to make vehicles follow specific trajectories and 
                    traffic rules. On the other hand, data-driven approaches generate realistic and human-like behaviors, improving 
                    transfer from simulated to real-world traffic. However, to the best of our knowledge, no traffic model offers 
                    both controllability and realism. In this work, we develop a conditional diffusion model for controllable traffic 
                    generation (CTG) that allows users to control desired properties of trajectories at test time (e.g., reach a goal 
                    or follow a speed limit) while maintaining realism and physical feasibility through enforced dynamics. The key 
                    technical idea is to leverage recent advances from diffusion modeling and differentiable logic to guide generated 
                    trajectories to meet rules defined using signal temporal logic (STL). We further extend guidance to multi-agent 
                    settings and enable interaction-based rules like collision avoidance. CTG is extensively evaluated on the nuScenes 
                    dataset for diverse and composite rules, demonstrating improvement over strong baselines in terms of the 
                    controllability-realism tradeoff.</p>
                <a class="btn btn-primary" href="https://aiasd.github.io/ctg.github.io/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://arxiv.org/abs/2210.17366">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://arxiv.org/abs/2210.17366">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/ctg_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>SpOT: Spatiotemporal Modeling for 3D Object Tracking</h3>
                <h4>C. Stearns, <b>D. Rempe</b>, J. Li, R. Ambrus, S. Zakharov, V. Guizilini, Y. Yang, and L. Guibas. European Conference on Computer Vision (<b>ECCV</b>), 2022.</h4> <h4 style="color:red;">Oral Presentation</h4>
                <p>3D multi-object tracking aims to uniquely and consistently
                    identify all mobile entities through time. Despite the rich spatiotem-
                    poral information available in this setting, current 3D tracking meth-
                    ods primarily rely on abstracted information and limited history, e.g.
                    single-frame object bounding boxes. In this work, we develop a holistic
                    representation of traffic scenes that leverages both spatial and tempo-
                    ral information of the actors in the scene. Specifically, we reformulate
                    tracking as a spatiotemporal problem by representing tracked objects as
                    sequences of time-stamped points and bounding boxes over a long tem-
                    poral history. At each timestamp, we improve the location and motion
                    estimates of our tracked objects through learned refinement over the
                    full sequence of object history. By considering time and space jointly,
                    our representation naturally encodes fundamental physical priors such
                    as object permanence and consistency across time. Our spatiotemporal
                    tracking framework achieves state-of-the-art performance on the Waymo
                    and nuScenes benchmarks.</p>
                <a class="btn btn-primary" href="https://github.com/coltonstearns/SpOT">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://arxiv.org/pdf/2207.05856.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://arxiv.org/pdf/2207.05856.pdf">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/spot_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior</h3>
                <h4><b>D. Rempe</b>, J. Philion, L. Guibas, S. Fidler, and O. Litany. Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</h4>
                <p>Evaluating and improving planning for autonomous vehicles requires scalable generation of long-tail traffic scenarios. To be useful, these scenarios must be realistic and challenging, but not impossible to drive through safely. In this work, we introduce STRIVE, a method to automatically generate challenging scenarios that cause a given planner to produce undesirable behavior, like collisions. To maintain scenario plausibility, the key idea is to leverage a learned model of traffic motion in the form of a graph-based conditional VAE. Scenario generation is formulated as an optimization in the latent space of this traffic model, perturbing an initial real-world scene to produce trajectories that collide with a given planner. A subsequent optimization is used to find a "solution" to the scenario, ensuring it is useful to improve the given planner. Further analysis clusters generated scenarios based on collision type. We attack two planners and show that STRIVE successfully generates realistic, challenging scenarios in both cases. We additionally "close the loop" and use these scenarios to optimize hyperparameters of a rule-based planner.</p>
                <a class="btn btn-primary" href="https://nv-tlabs.github.io/STRIVE/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://nv-tlabs.github.io/STRIVE/docs/strive.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://nv-tlabs.github.io/STRIVE/docs/strive.pdf">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/strive_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>HuMoR: 3D Human Motion Model for Robust Pose Estimation</h3>
                <h4><b>D. Rempe</b>, T. Birdal, A. Hertzmann, J. Yang, S. Sridhar, and L. Guibas. International Conference on Computer Vision (<b>ICCV</b>), 2021.</h4> <h4 style="color:red;">Oral Presentation</h4>
                <p> We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos.</p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/humor/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/humor/docs/humor.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://geometry.stanford.edu/projects/humor/docs/humor.pdf">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/humor_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries</h3>
                <h4>A. Kashefi, <b>D. Rempe</b>, and L. Guibas. Physics of Fluids, 2021.</h4>
                <p> We present a novel deep learning framework for flow field predictions in irregular domains when the solution is a function of the geometry of either the domain or objects inside the domain. Grid vertices in a computational fluid dynamics (CFD) domain are viewed as point clouds and used as inputs to a neural network based on the PointNet architecture, which learns an end-to-end mapping between spatial positions and CFD quantities. Using our approach, (i) the network inherits desirable features of unstructured meshes (e.g., fine and coarse point spacing near the object surface and in the far field, respectively), which minimizes network training cost; (ii) object geometry is accurately represented through vertices located on object boundaries, which maintains boundary smoothness and allows the network to detect small changes between geometries and (iii) no data interpolation is utilized for creating training data; thus accuracy of the CFD data is preserved. None of these features are achievable by extant methods based on projecting scattered CFD data into Cartesian grids and then using regular convolutional neural networks. Incompressible laminar steady flow past a cylinder with various shapes for its cross section is considered. The mass and momentum of predicted fields are conserved. We test the generalizability of our network by predicting the flow around multiple objects as well as an airfoil, even though only single objects and no airfoils are observed during training. The network predicts the flow fields hundreds of times faster than our conventional CFD solver, while maintaining excellent to reasonable accuracy. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/cfd_pointnet/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://aip.scitation.org/doi/10.1063/5.0033376">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="https://geometry.stanford.edu/projects/cfd_pointnet">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/neural_cfd_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations</h3>
                <h4><b>D. Rempe</b>, T. Birdal, Y. Zhao, Z. Gojcic, S. Sridhar, and L. Guibas. Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2020.</h4> <h4 style="color:red;">Spotlight Presentation</h4>
                <p> We propose CaSPR, a method to learn object-centric canonical spatiotemporal point cloud representations of dynamically moving or evolving objects. Our goal is to enable information aggregation over time and the interrogation of object state at any spatiotemporal neighborhood in the past, observed or not. Different from previous work, CaSPR learns representations that support spacetime continuity, are robust to variable and irregularly spacetime-sampled point clouds, and generalize to unseen object instances. Our approach divides the problem into two subtasks. First, we explicitly encode time by mapping an input point cloud sequence to a spatiotemporally-canonicalized object space. We then leverage this canonicalization to learn a spatiotemporal latent representation using neural ordinary differential equations and a generative model of dynamically evolving shapes using continuous normalizing flows. We demonstrate the effectiveness of our method on several applications including shape reconstruction, camera pose estimation, continuous spatiotemporal sequence reconstruction, and correspondence estimation from irregularly or intermittently sampled observations. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/caspr/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/caspr/content/CaSPR_CR.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br>
                <a href="docs/CaSPR_CR.pdf">
                    </br> <img class="img-responsive" width="100%" height="100%" src="images/caspr_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Contact and Human Dynamics from Monocular Video</h3>
                <h4><b>D. Rempe</b>, L. Guibas, A. Hertzmann, B. Russell, R. Villegas, and J. Yang. European Conference on Computer Vision (<b>ECCV</b>), 2020.</h4> <h4 style="color:red;">Spotlight Presentation</h4>
                <p> Existing deep models predict 2D and 3D kinematic poses from video that are approximately accurate, but contain visible errors that violate physical constraints, such as feet penetrating the ground and bodies leaning at extreme angles. In this paper, we present a physics-based method for inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input. We first estimate ground contact timings with a novel prediction network which is trained without hand-labeled data. A physics-based trajectory optimization then solves for a physically-plausible motion, based on the inputs. We show this process produces motions that are significantly more realistic than those from purely kinematic methods, substantially improving quantitative measures of both kinematic and dynamic plausibility. We demonstrate our method on character animation and pose estimation tasks on dynamic motions of dancing and sports with complex contact patterns. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/human-dynamics-eccv-2020/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/human-dynamics-eccv-2020/content/contact-and-dynamics-2020.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br></br>
                <a href="docs/contact-and-dynamics-2020.pdf">
                    </br> <img class="img-responsive" width="130%" height="130%" src="images/human_dynamics_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Predicting the Physical Dynamics of Unseen 3D Objects</h3>
                <h4><b>D. Rempe</b>, S. Sridhar, H. Wang, and L. Guibas. Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2020.</h4>
                <p> Machines that can predict the effect of physical interactions on the dynamics of previously unseen object instances are important for creating better robots, autonomous vehicles, and interactive virtual worlds. In this work, we focus on predicting the dynamics of 3D objects on a plane that have just been subjected to an impulsive force. In particular, we predict the changes in state---3D position, rotation, velocities, and stability. Different from previous work, our approach can generalize dynamics predictions to object shapes and initial conditions that were unseen during training. Our method takes the 3D object's shape as a point cloud and its initial linear and angular velocities as input. We extract shape features and use a recurrent neural network to predict the full change in state at each time step. Our model can support training with data from both a physics engine or the real world. Experiments show that we can accurately predict the changes in state for unseen object geometries and initial conditions. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/learningdynamicsWACV2020/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/learningdynamicsWACV2020/content/Learning_Dynamics_WACV_2020.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br></br>
                <a href="docs/Learning_Dynamics_WACV_preprint.pdf">
                    </br> <img class="img-responsive" src="images/wacv_dynamics_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Multiview Aggregation for Learning Category-Specific Shape Reconstruction</h3>
                <h4>S. Sridhar, <b>D. Rempe</b>, J. Valentin,  S. Bouaziz, and L. Guibas. Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2019.</h4>
                <p> We investigate the problem of learning category-specific 3D shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations, or assume a fixed number of views. We present a method that can estimate dense 3D shape, and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance, we propose a representation that encodes the dense shape of the visible object surface as well as the surface behind line of sight occluded by the visible surface. When multiple input views are available, the shape representation is designed to be aggregated into a single 3D shape using an inexpensive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense 3D reconstructions of objects that improve in quality as more views are added. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/xnocs/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/xnocs/pub/xnocs.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br></br>
                <a href="https://geometry.stanford.edu/projects/xnocs/">
                    </br> <img class="img-responsive" src="images/xnocs_teaser.png" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Learning Generalizable Final-State Dynamics of 3D Rigid Objects</h3>
                <h4><b>D. Rempe</b>, S. Sridhar, H. Wang, and L. Guibas. <b>CVPR Workshop</b> on 3D Scene Understanding for Vision, Graphics, and Robotics, 2019.</h4>
                <p> Humans have a remarkable ability to predict the effect of physical interactions on the dynamics of objects. Endowing machines with this ability would allow important applications in areas like robotics and autonomous vehicles. In this work, we focus on predicting the dynamics of 3D rigid objects, in particular an object's final resting position and total rotation when subjected to an impulsive force. Different from previous work, our approach is capable of generalizing to unseen object shapes---an important requirement for real-world applications. To achieve this, we represent object shape as a 3D point cloud that is used as input to a neural network, making our approach agnostic to appearance variation. The design of our network is informed by an understanding of physical laws. We train our model with data from a physics engine that simulates the dynamics of a large number of shapes. Experiments show that we can accurately predict the resting position and total rotation for unseen object geometries. </p>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/learningdynamics/">Project Page <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://geometry.stanford.edu/projects/learningdynamics/content/Dynamics_CVPR_Workshop_CamReady.pdf">Workshop Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="https://arxiv.org/abs/1901.00466">Full Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
                <a href="https://geometry.stanford.edu/projects/learningdynamics/">
                    </br> <img class="img-responsive" src="images/learn_dynamics.png" alt="">
                </a>
            </div>
        </div>

        <hr>

		<div class="row">
            <div class="col-md-8">
                <h3>Effectiveness of Global, Low-Degree Polynomial Transformations for GCxGC Data Alignment</h3>
                <h4><b>D. Rempe</b>, S. Reichenbach, Q. Tao, C. Cordero, W. Rathbun, and C.A. Zini. <b>Analytical Chemistry</b>, 88(20), pp. 10028-10035, 2016.</h4>
                <p> As columns age and differ between systems, retention times for comprehensive two-dimensional gas chromatography (GCxGC) may
				vary between runs. In order to properly analyze GCxGC chromatograms, it often is desirable to align the retention times of
				chromatographic features, such as analyte peaks, between chromatograms. Previous work by the authors has shown that global, low-degree
				polynomial transformation functions – namely affine, second-degree polynomial, and third-degree polynomial – are effective for aligning
				pairs of two-dimensional chromatograms acquired with dual second columns and detectors (GCx2GC). This work assesses the experimental performance
				of these global methods on more general GCxGC chromatogram pairs and com- pares their performance to that of a recent, robust, local
				alignment algorithm for GCxGC data [Gros et al., Anal. Chem. 2012, 84, 9033]. Measuring performance with the root-mean-square (RMS) residual
				differences in retention times for matched peaks suggests that global, low-degree polynomial transformations outperform the local algorithm
				given a sufficiently large set of alignment points, and are able to improve misalignment by over 95% based on a lower-bound benchmark of inherent
				variability. However, with small sets of alignment points, the local method demonstrated lower error rates (although with greater computational
				overhead). For GCxGC chromatogram pairs with only slight initial misalignment, none of the global or local methods performed well. In some cases
				with initial misalignment near the inherent variability of the system, these methods worsened alignment, suggesting that it may be better not to
				perform alignment in such cases. </p>
                <a class="btn btn-primary" href="docs/publishedAC_091816.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="docs/publishedAC_091816_supportingInfo.pdf">Supporting Info <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
            </br></br></br>
                <a href="docs/publishedAC_091816.pdf">
                    </br> <img class="img-responsive" src="images/alignmentTOC_gcxgc.gif" alt="">
                </a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8">
                <h3>Alignment for Comprehensive Two-Dimensional Gas Chromatography with Dual Secondary Columns and Detectors</h3>
                <h4>S. Reichenbach, <b>D. Rempe</b>, Q. Tao, D. Bressanello, E. Liberto, C. Bicchi, S. Balducci, and C. Cordero. <b>Analytical Chemistry</b>, 87(19), pp. 10056-10063, 2015.</h4>
                <p> In each sample run, comprehensive two-dimensional gas chromatography with dual secondary columns and detectors (GC × 2GC) provides
                    complementary information in two chromatograms generated by its two detectors. For example, a flame ionization detector (FID) produces
                    data that is especially effective for quantification and a mass spectrometer (MS) produces data that is especially useful for chemical-structure
                    elucidation and compound identification. The greater information capacity of two detectors is most useful for difficult analyses,
                    such as metabolomics, but using the joint information offered by the two complex two-dimensional chromatograms requires data fusion.
                    In the case that the second columns are equivalent but flow conditions vary (e.g., related to the operative pressure of their different detectors),
                    data fusion can be accomplished by aligning the chromatographic data and/or chromatographic features such as peaks and retention-time windows.
                    Chromatographic alignment requires a mapping from the retention times of one chromatogram to the retention times of the other chromatogram.
                    This paper considers general issues and experimental performance for global two-dimensional mapping functions to align pairs of GC × 2GC chromatograms.
                    Experimental results for GC × 2GC with FID and MS for metabolomic analyses of human urine samples suggest that low-degree polynomial mapping functions
                    out-perform affine transformation (as measured by root-mean-square residuals for matched peaks) and achieve performance near a lower-bound benchmark
                    of inherent variability. Third-degree polynomials slightly out-performed second-degree polynomials in these results, but second-degree polynomials
                    performed nearly as well and may be preferred for parametric and computational simplicity as well as robustness. </p>
                <a class="btn btn-primary" href="docs/publishedAC_092215.pdf">Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
                <a class="btn btn-primary" href="docs/publishedAC_092215_supportingInfo.pdf">Supporting Info <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
            <div class="col-md-4">
                <a href="docs/publishedAC_092215.pdf">
                    </br> <img class="img-responsive" src="images/screenshot_gcx2gc.gif" alt=""> </br>
                    <img class="img-responsive" src="images/screenshot2_gcx2gc.gif" alt="">
                </a>
            </div>
        </div>

        <hr>

        <!-- /.row -->
        <div class="row">
            <div class="col-md-12">
                <h3>PhD Thesis</h3>
                <h4>Advised by Leonidas Guibas</h4>
                <a class="btn btn-primary" href="docs/Rempe_thesis_final-published.pdf">Thesis <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-12">
                <h3>Undergraduate Thesis</h3>
                <h4>Advised by Stephen Scott and Stephen Reichenbach</h4>
                <a class="btn btn-primary" href="docs/distinction-thesis.pdf">Thesis <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
        </div>

        <hr>

        <!-- Page Heading -->
        <!-- <div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">Other Publications and Presentations
                </h1>
                <h4>(<u>underline</u> indicates the presenter/s)</h4>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h3>A Cognitive Radio TV Prototype for Effective TV Spectrum Sharing</h3>
                <h4><b>D. Rempe</b>, M. Snyder, A. Pracht, A. Schwarz, T. Nguyen, <u>M. Vostrez</u>, <u>Z. Zhao</u>, and M.C. Vuran. 2017 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN) (IEEE DySPAN 2017), Baltimore, MD, USA, March 6-9, 2017.</h4>
                <a class="btn btn-primary" href="docs/dyspan2017_final_published.pdf">Demo Paper <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
        </div>

        <hr> -->

        <!-- <div class="row">
            <div class="col-md-12">
                <h3>Simple models for second-column retention-time variability across peaks from GCxGC</h3>
                <h4> S. Reichenbach, <u><b>D. Rempe</b></u>, Q. Tao, and C. Cordero, 8th Multidimensional Chromatography Workshop, Toronto, ON, Canada, January 5-6, 2017.</h4>
                <a class="btn btn-primary" href="docs/MDCW_20161229_4-3_noAnim.pdf">Slides <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
        </div>

        <hr> -->

		<!-- <div class="row">
            <div class="col-md-12">
                <h3>Alignment for Comprehensive Two-Dimensional Gas Chromatography (GCxGC) with Global, Low-Order Polynomial Transformations</h3>
                <h4><u><b>D. Rempe</b></u>, S. Reichenbach, and S. Scott, UNL Spring Research Fair Poster Session, Lincoln, NE, USA, April, 2016.</h4>
                <a class="btn btn-primary" href="docs/UNL_poster_session.pdf">Poster <span class="glyphicon glyphicon-chevron-right"></span></a>
            </div>
        </div>

        <hr> -->

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
